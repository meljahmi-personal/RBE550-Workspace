# RBE550 Grid Bench â€“ Search Benchmark & RViz Demo

This repository contains my final RBEâ€‘550 project: a **2â€‘D gridworld search benchmark**
with RViz visualization and optional Docker support.

Core features

- Multiple planners: **BFS, Dijkstra, Greedy Bestâ€‘First, A\*, Weighted A\*, Theta\***, **JPS**
- Random and fixed maps (32Ã—32 / 64Ã—64, maze and random obstacle fields)
- Metrics: runtime, nodes expanded, path length, number of turns
- ROSÂ 2 RViz visualization with animated â€œrobotâ€ marker
- Batch benchmarking on host or inside Docker (same code path)

The instructions below are written for reproducibility from a clean clone on Ubuntu 22.04.

---

## 1. Host setup: clone, dependencies, and build

These steps assume **UbuntuÂ 22.04** with a graphical desktop.

### 1.1 Clone the repository

From any directory you like:

```bash
git clone https://github.com/meljahmi-personal/RBE550-Workspace.git
cd RBE550-Workspace
```

This is a standard ROSÂ 2 workspace (with `src/`, `scripts/`, `Dockerfile`, etc.).
No Python virtual environment is used or required.

### 1.2 Install system dependencies (ROSÂ 2 + RViz + tools)

Install ROSÂ 2 Humble and basic tools (if not already installed):

```bash
sudo apt update
sudo apt install ros-humble-desktop python3-colcon-common-extensions git build-essential
```

The `ros-humble-desktop` metaâ€‘package includes **RViz**.

### 1.3 Install Python dependencies (for benchmarking and plotting)

The benchmarking and plotting scripts use NumPy, pandas, and Matplotlib.

```bash
sudo apt install python3-pip
pip3 install --user numpy pandas matplotlib
```

Again: **no virtual environment** is created by this project; the system
Python from Ubuntu 22.04 is used.

### 1.4 Build the workspace

From the repository root (`RBE550-Workspace`):

```bash
./scripts/build.sh
```

This script:

- cleans the workspace (via `colcon`),
- sources `/opt/ros/humble/setup.bash`,
- runs `colcon build --symlink-install`.

After a successful build you can either manually source:

```bash
source /opt/ros/humble/setup.bash
source install/setup.bash
```

or simply run:

```bash
./scripts/activate.sh
```

which performs both steps and prints the available ROSÂ 2 commands.

---

## 2. Running from the host (RViz demo + benchmarks)

This section assumes you are **not** using Docker and have completed
SectionÂ 1 on the host.

### 2.1 RViz demo with animated robot (twoâ€‘terminal workflow)

The main visualization workflow uses **two terminals**.

#### TerminalÂ 1 â€“ build, activate, and launch RViz

From `RBE550-Workspace`:

```bash
./scripts/build.sh
./scripts/activate.sh
./scripts/run_rviz.sh
```

- `run_rviz.sh` sources ROSÂ 2 and the workspace and launches
  `bench.launch.py`, which starts:
  - the `planner_node` (implemented in `src/rbe550_grid_bench/rbe550_grid_bench/planner_node.py`)
  - RViz with the preconfigured layout `rbe550.rviz`

In RViz you should see:

- a **64Ã—64 occupancy grid** centered at the origin,
- obstacles as black cells,
- a shortest path drawn as a polyline,
- start (green sphere), goal (red sphere),
- a **large blue â€œrobotâ€ sphere** that animates along the path,
- a text overlay under the grid with the current algorithm, grid size,
  path length, and number of nodes expanded.

Keep this terminal running while you explore RViz.

#### TerminalÂ 2 â€“ feed different algorithms into RViz

Open a second terminal in `RBE550-Workspace` and run:

```bash
./scripts/activate.sh
# (the first time on a new machine you may also run ./scripts/build.sh here)
./scripts/feed_rviz_demo.sh
```

`feed_rviz_demo.sh`:

- waits for the `planner_node` started by `run_rviz.sh`,
- repeatedly calls the `randomize_grid` service,
- cycles through algorithms and settings,
- sleeps between calls so you can see RViz update.

This is the **exact setup** I used to collect the RViz screenshots
and to record a short video / GIF showing multiple planners and
maps over time.

**Important:** `./scripts/feed_rviz_demo.sh` is only meant to be run
**after** `./scripts/run_rviz.sh` is active in TerminalÂ 1.

### 2.2 Alternate RViz launchers for different maps

In addition to `run_rviz.sh` (64Ã—64 random map), there are three
convenience scripts:

```bash
# 64Ã—64 random map (same as run_rviz.sh)
./scripts/run_random64.sh

# 32Ã—32 random obstacle field
./scripts/run_random32.sh

# 32Ã—32 fixed maze (ASCII map)
./scripts/run_maze32.sh
```

All of these launch the same `planner_node` but with different
parameters (grid size, obstacle density, or `map_path`).

### 2.3 Hostâ€‘side benchmarks (CSV + plots)

All benchmarking is driven by one helper script that wraps
`rbe550_grid_bench.bench_runner`.

From `RBE550-Workspace`:

```bash
./scripts/activate.sh
./scripts/run_all_benchmarks.sh --seed 42 --plot
```

This command:

- sweeps algorithms (BFS, Dijkstra, Greedy, A\*, Weighted A\*, Theta\*, JPS),
- sweeps grid sizes (e.g., 32Ã—32 and 64Ã—64) and obstacle densities,
- logs metrics (runtime, nodes expanded, path length, number of turns),
- writes a consolidated CSV to:  
  `results/bench/bench_all.csv`
- generates PNG plots under:  
  `results/figs/`

ğŸ‘‰ **This exact command was used to generate all benchmark plots
that appear in the project report:**

```bash
./scripts/run_all_benchmarks.sh --seed 42 --plot
```

On a typical machine this completes quickly and produces a set of
figures suitable for direct inclusion in the writeâ€‘up.

---


#3. Docker Workflows

Docker support allows running the benchmarks without installing ROS 2 on the host.
The same code is used inside and outside Docker.

The main wrapper is:
```bash
./scripts/run_docker.sh
```

It supports three modes:

build â€“ build the image

bench â€“ run a single benchmark configuration

bench_all â€“ run the full multi-algorithm benchmark + plots

rviz â€“ (optional) try RViz from inside Docker (depends on X11/OpenGL setup)

#3.1 Build the Docker image

From RBE550-Workspace:

```bash
./scripts/run_docker.sh build
```

This:

- uses the provided Dockerfile

- installs ROS 2 Humble, RViz2, colcon

- installs Python dependencies (NumPy, pandas, Matplotlib)

- builds and installs rbe550_grid_bench under /ws in the image

- tags the image as rbe550-bench:latest (via the script)

- You only need to rebuild if you change the source code.


# 3.2 Single benchmark run inside Docker

Example: run A* once on a 64Ã—64 grid with 8-connected moves, no GUI:

```bash
./scripts/run_docker.sh bench --algo astar --grid 64 --moves 8 --steps 1 --no-show
```

This:

- starts a container

- runs the internal CLI (bench) with your arguments

- prints metrics in the host terminal

# 3.3 Reproducing all plots inside Docker

To reproduce the full benchmark CSV and all plots entirely inside Docker,
using the same settings as the host-side run:

```bash
./scripts/run_docker.sh build           
./scripts/run_docker.sh bench_all
```

and writes the same files to results/ on the host:

- results/bench_all.csv

- results/bench_runtime_ms.png

- results/bench_nodes_expanded.png

- results/bench_path_len.png

- results/bench_memory_nodes.png

- results/bench_path_turns.png

- results/bench_efficiency_scatter.png

- results/bench_comprehensive_dashboard.png

# 3.4 Optional: RViz from within Docker

On some systems you can also run RViz inside Docker, for example:

```bash
xhost +local:root   # allow local docker GUI (X11) access
./scripts/run_docker.sh build
./scripts/run_docker.sh rviz algo:=astar grid:=64 moves:=8
```

This will:

run the same planner_node as the host RViz workflow

launch RViz in the container, displaying the grid and path

Important compatibility note:
RViz inside Docker depends on the hostâ€™s X11, GPU drivers, and OpenGL
passthrough. On my machine (Ubuntu 22.04 with working X11/OpenGL),
this configuration runs successfully. On other hardware/driver
combinations, RViz may fail to display or may log OpenGL-related warnings.

The host-side RViz workflow in Section 2.2 is the primary and most
reliable visualization path. Docker is primarily intended for running
benchmarks and reproducing the CSV/plots.

---

# 4. Repository Layout (High Level)

RBE550-Workspace/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ rbe550_grid_bench/             # ROS 2 package (planning + benchmarking)
â”‚       â”œâ”€â”€ launch/
â”‚       â”‚   â””â”€â”€ bench.launch.py        # RViz launch entrypoint
â”‚       â”œâ”€â”€ config/
â”‚       â”‚   â””â”€â”€ rbe550.rviz            # RViz display configuration
â”‚       â”œâ”€â”€ rbe550_grid_bench/
â”‚       â”‚   â”œâ”€â”€ algorithms/            # BFS, Dijkstra, Greedy, A*, Weighted A*,
â”‚       â”‚   â”‚                          # Theta*, JPS implementations
â”‚       â”‚   â”œâ”€â”€ bench_runner.py        # Central benchmarking dispatcher
â”‚       â”‚   â”œâ”€â”€ cli.py                 # `ros2 run ... bench` CLI
â”‚       â”‚   â”œâ”€â”€ grid_utils.py          # Random grids, ASCII maps, sampling
â”‚       â”‚   â”œâ”€â”€ neighbors.py           # 4-connected and 8-connected neighbor sets
â”‚       â”‚   â”œâ”€â”€ metrics.py             # Runtime, path length, memory, CSV writer
â”‚       â”‚   â”œâ”€â”€ planner_node.py        # RViz visualization node
â”‚       â”‚   â”œâ”€â”€ plot_bench_all.py      # Plotting backend for benchmarks
â”‚       â”‚   â”œâ”€â”€ rviz_helpers.py        # Helper functions for RViz markers
â”‚       â”‚   â””â”€â”€ maps/                  # Optional ASCII test maps (unused in release)
â”‚       â”œâ”€â”€ package.xml
â”‚       â”œâ”€â”€ setup.py
â”‚       â””â”€â”€ test/                      # linters + style checks
â”‚
â”œâ”€â”€ scripts/                           # Executable workflows
â”‚   â”œâ”€â”€ build.sh                       # Clean + colcon build
â”‚   â”œâ”€â”€ activate.sh                    # Source install/setup.bash automatically
â”‚   â”œâ”€â”€ run_rviz.sh                    # Host-side RViz launch wrapper
â”‚   â”œâ”€â”€ feed_rviz_demo.sh              # Timed simulation publisher
â”‚   â”œâ”€â”€ run_random32.sh                # Convenience: 32Ã—32 random grid
â”‚   â”œâ”€â”€ run_random64.sh                # Convenience: 64Ã—64 random grid
â”‚   â”œâ”€â”€ run_maze32.sh                  # Run on maze_32 ASCII map
â”‚   â”œâ”€â”€ run_all_benchmarks.sh          # Full 7-algorithm benchmark suite
â”‚   â”œâ”€â”€ run_docker.sh                  # Docker build/bench/rviz frontend
â”‚   â”œâ”€â”€ run_check.sh                   # Quick development smoke test
â”‚   â”œâ”€â”€ entrypoint.sh                  # Docker container entrypoint
â”‚   â”œâ”€â”€ start.sh                       # Developer helper
â”‚   â””â”€â”€ dev/                           # Internal development/testing helpers
â”‚       â”œâ”€â”€ test_no_rviz.sh
â”‚       â”œâ”€â”€ test_with_rviz.sh
â”‚       â”œâ”€â”€ demo_flicker.sh
â”‚       â””â”€â”€ run_demo.sh
â”‚
â”œâ”€â”€ maps/
â”‚   â”œâ”€â”€ maze_32.txt                    # 32Ã—32 ASCII maze (handcrafted)
â”‚   â””â”€â”€ maze_32_32.txt                 # Duplicate of the above; kept for testing
â”‚                                      # (both contain ASCII obstacle maps)
â”‚
â”œâ”€â”€ results/                           # Auto-generated: CSVs + PNG plots
â”‚   â”œâ”€â”€ bench_all.csv
â”‚   â”œâ”€â”€ bench_runtime_ms.png
â”‚   â”œâ”€â”€ bench_nodes_expanded.png
â”‚   â”œâ”€â”€ bench_path_len.png
â”‚   â”œâ”€â”€ bench_memory_nodes.png
â”‚   â”œâ”€â”€ bench_path_turns.png
â”‚   â””â”€â”€ bench_comprehensive_dashboard.png
â”‚
â”œâ”€â”€ outputs/                           # Raw text logs from CLI runs
â”‚   â””â”€â”€ run_*.log
â”‚
â”œâ”€â”€ architecture_diagrams/             # PlantUML source + exported diagrams
â”‚   â”œâ”€â”€ architecture.puml
â”‚   â”œâ”€â”€ class_core.puml
â”‚   â”œâ”€â”€ class_algorithms.puml
â”‚   â”œâ”€â”€ sequence_bench_all.puml
â”‚   â””â”€â”€ *.svg / *.png
â”‚
â”œâ”€â”€ Dockerfile                         # Self-contained ROS 2 image
â”œâ”€â”€ README.md
â””â”€â”€ video/
    â””â”€â”€ multiple_algorithms.mkv        # Demonstration recording


---

# 5. Notes on Portability

This project was tested thoroughly on:

Ubuntu 22.04

ROS 2 Humble

Native host execution (CLI + RViz)

Docker execution (benchmarks + plots, and RViz with working X11/OpenGL)

While the core algorithms and scripts are deterministic, actual behavior of:

Docker-side RViz, and

any GPU-accelerated rendering

can vary across distributions, GPU drivers, and Docker/X11/OpenGL setups.
If a particular combination fails, the recommended approach is to:

run all benchmarks using run_all_benchmarks.sh on the host or via bench_all in Docker

use the host-side RViz workflow for visualization.



